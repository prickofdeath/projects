{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Applied Linear Regression - Student.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunwhyemun/projects/blob/master/Copy_of_Applied_Linear_Regression_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaTU6AoPv5W5",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression in Practice\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFueTl6TwH-r",
        "colab_type": "text"
      },
      "source": [
        "## Background\n",
        "\n",
        "**Regression problems** are supervised learning problems in which the target output variable is continuous. Contrast this with **Classification problems** (which we will treat separately), which are supervised learning problems in which the target is categorical in nature. \n",
        "\n",
        "**Linear regression** is a technique that is useful for regression problems. It is a simplistic way to model relationships with well known results (if certain assumptions are met).\n",
        "\n",
        "So, why are linear regression models useful in practice?\n",
        "\n",
        "- runs fast\n",
        "- easy to use (not a lot of tuning required)\n",
        "- highly interpretable\n",
        "- sets a good benchmark for other machine learning methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7kQi3JMwbDM",
        "colab_type": "text"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "We'll be using a combination of the [scikit-learn](http://scikit-learn.org/stable/) library for prediction and the [Statsmodels](http://statsmodels.sourceforge.net/) library for analysis of the models. Linear Models have some nice inference results that appear if some assumptions - such as normal, i.i.d errors - are satisfied. \n",
        "\n",
        "However, these assumptions might often fail in practice. Thus, we will be spending most of your energy on the former since it provides significantly more useful functionality for machine learning in general.\n",
        "\n",
        "We will also be using pandas library and some plotting tools for exploratory data analysis and data cleaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo_F7Wb0tfA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.formula.api as sm\n",
        "import statsmodels.api as sma\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# allow plots to appear directly in the notebook\n",
        "%matplotlib inline\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQomYBJXuoVT",
        "colab_type": "text"
      },
      "source": [
        "## Case Study: Advertising Data\n",
        "\n",
        "Let's explore some data related to marketing and sales, and then use linear regression to answer those questions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6Vz-T7xvFc9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c0de1491-8ddb-4e65-83d7-9c5a85e1b59d"
      },
      "source": [
        "# read data into a DataFrame\n",
        "\n",
        "## Recall that pandas.read_csv is a function that loads data into a dataframe. \n",
        "## The data can be represented as a file on your computer or even a file that can be pulled from an online source.\n",
        "\n",
        "data = pd.read_csv('http://faculty.marshall.usc.edu/gareth-james/ISL/Advertising.csv', index_col=0)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TV</th>\n",
              "      <th>radio</th>\n",
              "      <th>newspaper</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>230.1</td>\n",
              "      <td>37.8</td>\n",
              "      <td>69.2</td>\n",
              "      <td>22.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44.5</td>\n",
              "      <td>39.3</td>\n",
              "      <td>45.1</td>\n",
              "      <td>10.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17.2</td>\n",
              "      <td>45.9</td>\n",
              "      <td>69.3</td>\n",
              "      <td>9.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>151.5</td>\n",
              "      <td>41.3</td>\n",
              "      <td>58.5</td>\n",
              "      <td>18.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>180.8</td>\n",
              "      <td>10.8</td>\n",
              "      <td>58.4</td>\n",
              "      <td>12.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      TV  radio  newspaper  sales\n",
              "1  230.1   37.8       69.2   22.1\n",
              "2   44.5   39.3       45.1   10.4\n",
              "3   17.2   45.9       69.3    9.3\n",
              "4  151.5   41.3       58.5   18.5\n",
              "5  180.8   10.8       58.4   12.9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtXdAVrTvO83",
        "colab_type": "text"
      },
      "source": [
        "What are the **features** (i.e the predictor/input variables)?\n",
        "- TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
        "- Radio: advertising dollars spent on Radio\n",
        "- Newspaper: advertising dollars spent on Newspaper\n",
        "\n",
        "What is the **response** (i.e the target/output variable)?\n",
        "- Sales: sales of a single product in a given market (in thousands of widgets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sX_TrvZwCjw",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaXb0eniwIj3",
        "colab_type": "text"
      },
      "source": [
        "Let's get a summary of the data and all the nuances:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oTOgLlFvXJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the shape of the DataFrame\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DzOJfzFv9QA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a high level overview of the data:\n",
        "## Information includes column name, column types and any potential missing values.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mssal3lTwP-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a high level summary of each column\n",
        "\n",
        "## Since the columns are numeric in nature, we can get a summary of aggregates for each column.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpy_AvOgwRbi",
        "colab_type": "text"
      },
      "source": [
        "There seem to be no missing values and no illogical values (i.e negative quantities), which is a good sign of a clean dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Hn1QexyQ47",
        "colab_type": "text"
      },
      "source": [
        "Let's now use some plots to visualise the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQPXnJ_Ax0BO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the relationship between the features and the response using scatterplots\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R7wJYRCyT15",
        "colab_type": "text"
      },
      "source": [
        "What can you tell from this dataset so far?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhY3zNxyylUf",
        "colab_type": "text"
      },
      "source": [
        "## Questions About the Advertising Data\n",
        "\n",
        "Let's pretend you work for the company that manufactures and markets this widget. The company might ask you the following: On the basis of this data, how should we spend our advertising money in the future?\n",
        "\n",
        "This general question might lead you to more specific questions:\n",
        "1. Is there a relationship between ads and sales?\n",
        "2. How strong is that relationship?\n",
        "3. Which ad types contribute to sales?\n",
        "4. What is the effect of each ad type of sales?\n",
        "5. Given ad spending in a particular market, can sales be predicted? To what degree of error?\n",
        "\n",
        "We will explore these questions below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TdMNQ44ypPt",
        "colab_type": "text"
      },
      "source": [
        "Given that the diagrams above show some kind of correlation between sales and money spent on advertising, we can try to model a linear relationship between sales and the different types of ad spending:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfFskfn1z46V",
        "colab_type": "text"
      },
      "source": [
        "## Simple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3rTBMaVWpiY",
        "colab_type": "text"
      },
      "source": [
        "Recall we can impose a linear regression model for predicting the (continous) **response** using the **predictor variables** (or \"input variables\"). It takes the following form:\n",
        "\n",
        "$y^{pred}_i = \\sum_{j=0}^{k} \\beta_jX_{ij}$\n",
        "\n",
        "Where the representation of the terms are as follows:\n",
        "- $y_i$ is the response of the $i$th observed variable\n",
        "- $X_{ij}$ corresponds to the $j$th predictor value of the $i$th observed set of predictors\n",
        "- $\\beta_0$ is the intercept, (and $X_{i0}$ is fixed to be 1 for all $i$)\n",
        "- $\\beta_j$ are the coefficients for each predictor.\n",
        "- Also, assume that there are $k$ predictors. In the setup for the marketing data, $k = 3$\n",
        "\n",
        "Together, the $\\beta_i$ are called the **model coefficients**. To create your model, you must \"learn\" the values of these coefficients. And once we've learned these coefficients, we can use the model to predict Sales!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfbAbnrX0gaB",
        "colab_type": "text"
      },
      "source": [
        "Recall as well we are trying to minimise the MSE with the gradient descent algorithm. Rather than trying to put all of these specification into code ourselves, we can take advantage of the `scikit-learn` or `statsmodels` libraries to do so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgG9Ojhz11pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### STATSMODELS ###\n",
        "\n",
        "# create a fitted model on the tv ad sales alone\n",
        "\n",
        "\n",
        "# print the coefficients\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw-fuGJr16nm",
        "colab_type": "text"
      },
      "source": [
        "We can also easily do this with `scikit-learn`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akoCbPds2IKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### SCIKIT-LEARN ###\n",
        "\n",
        "# create X and y\n",
        "\n",
        "\n",
        "# instantiate and fit\n",
        "\n",
        "\n",
        "# print the coefficients\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VantvANb2N3m",
        "colab_type": "text"
      },
      "source": [
        "### Interpreting Model Coefficients\n",
        "\n",
        "How do we interpret the TV coefficient ($\\beta_1$)?\n",
        "\n",
        "If the model were correct:\n",
        "\n",
        "- A \"unit\" increase in TV ad spending is **associated with** a 0.04737 \"unit\" increase in Sales.\n",
        "- Or more clearly: An additional $1,000 spent on TV ads is **associated with** an increase in sales of around 47 widgets.\n",
        "\n",
        "Note that if an increase in TV ad spending was associated with a **decrease** in sales, $\\beta_1$ would be **negative**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssK0K4oY6k9x",
        "colab_type": "text"
      },
      "source": [
        "We could plot this out to gain a graphical overview:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8rXn80j2mkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv3A_f1_3JB0",
        "colab_type": "text"
      },
      "source": [
        "### Using the Model for Prediction\n",
        "\n",
        "Let's say that there was a new market where the TV advertising spend was **$100,000**. What would we predict for the Sales in that market?\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1x$$\n",
        "$$y = 7.032594 + 0.047537 \\times 100$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhF5xXSo_z_h",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1W8tQsH_unF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_linear(x,slope,intercept):\n",
        "    \"\"\"\n",
        "    Predict the output of a linear model.\n",
        "    \n",
        "    Inputs\n",
        "    x: an array of input predictor values\n",
        "    slope: an array of coefficients\n",
        "    intercept: a single numerical value representing the intercept term.\n",
        "\n",
        "    Outputs the estimate of the target output characterised by a linear model: a scalar.\n",
        "    \"\"\"\n",
        "    pass\n",
        "    \n",
        "try:\n",
        "  predict_linear(predict_linear([100],[lm2.coef_],lm2.intercept_)) #Should return 11.786;\n",
        "except NameError:\n",
        "  pass\n",
        "except TypeError:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67L_CFv8_3dz",
        "colab_type": "text"
      },
      "source": [
        "#### Prediction using libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elkaQD718z8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Using STATSMODELS ###\n",
        "\n",
        "# you have to create a DataFrame since the Statsmodels formula interface expects it\n",
        "\n",
        "\n",
        "# predict for a new observation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11ebOUkp86It",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### SCIKIT-LEARN ###\n",
        "\n",
        "# predict for a new observation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNqQncq7AROA",
        "colab_type": "text"
      },
      "source": [
        "Using the linear model, we would predict Sales of **9,566 widgets** in that market."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EAXXnc0Agob",
        "colab_type": "text"
      },
      "source": [
        "### Revisiting Least Squares Line\n",
        "\n",
        "As a matter of fact, seaborn has built in regression plots that gives us the least squares line automatically for the target versus each of the predictors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9Qzi-7vCmdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEfZskyKDFpA",
        "colab_type": "text"
      },
      "source": [
        "Note that in the plots above, the shaded regions represent the confidence intervals of the estimates. One can observe that regions where there is a high density of observations correspond to tighter confidence bounds for the estimates and vice versa for areas of lower observation density. Spread of the data points around the prediction line affects the confidence interval as well. This naturally brings us to ask the question, how well does my model perform?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPhCe1xtwbrE",
        "colab_type": "text"
      },
      "source": [
        "## Exploring the Bias-Variance Tradeoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_KQoi9mGTJ3",
        "colab_type": "text"
      },
      "source": [
        "Depending on the set up of your problem, the performance may be tied to different scoring mechanisms.\n",
        "\n",
        "- In the case of predictive modelling, an obvious gauge to whether the model performs well is the prediction error of the model, usually this is represented by the MSE.\n",
        "- In the case where we might be more interested in inference, we may be more interested in the statistical properties of the parameter estimates.\n",
        "\n",
        "So far, we have setup the machine learning problem such that we seek to find the parameter values that minimise the MSE. The key fact in this set up is that we are aiming to reduce the MSE for data that is **observed**.\n",
        "\n",
        "However, what a truly performant model seeks to achieve is not just the best predictions (i.e lowest MSE) on observed data, but rather the true value of the model comes from being able to make good predictions on data that is unseen. That is, we want to be able to make good out-of-sample predictions. \n",
        "\n",
        "*Remark: The concept of out-of-sample errors will be further elucidated in another section. Right now we will talk about errors generally.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLzZGm8e7MkM",
        "colab_type": "text"
      },
      "source": [
        "There are two performance characteristics impacted by the choice of the model:\n",
        "  - **Bias**: This can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function using a learning method for linear models, there will be error in the estimates due to this assumption. Models with high bias are, loosely speaking, said to *underfit* the data.\n",
        "  \n",
        "  - **Variance**: This can be understood as a measure of the amount that the estimate of the target function will change if different training data was used. Models with high variance are said to *overfit* the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBjIzeAf8Bef",
        "colab_type": "text"
      },
      "source": [
        "We want to find a model that minimises both bias and variance. However there is often a trade-off.\n",
        "\n",
        "Let's consider our one parameter linear model again. Instead of plotting `sales` as a function of the `tv` predictor, we will consider the deviations off the prediction line. These values are called (error) residuals:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cws76lU78aVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYb6Gfnv8hIp",
        "colab_type": "text"
      },
      "source": [
        "This model tends to seems to do better in terms of predictions for lower values of advertising spending. Towards higher values you can see the variance of the residuals (deviations of the data points from the prediction line) get larger. This means in general the linear model fails to capture the essence of this information simply due to the fact that we are missing on other factors that explain this variation. In this instance, we say that the model has *high bias* or has underfit the (observed) data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B-j71_cp8Vz",
        "colab_type": "text"
      },
      "source": [
        "In order to make sense of the model variance, one needs to consider what happens when the model has been trained on different sets of data. First let's group the data out by a random sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlD8PWIstV_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set a random seed for reproducibility\n",
        "\n",
        "\n",
        "# randomly assign every row to either sample 1, sample 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY_jFqsdFBAo",
        "colab_type": "text"
      },
      "source": [
        "We can now tell `Seaborn` to create two plots, in which the left plot only uses the data from sample 1 and the right plot only uses the data from sample 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWUGk9QbFjrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# col='sample' subsets the data by sample and creates two separate plots\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P56cu88lLOor",
        "colab_type": "text"
      },
      "source": [
        "The line looks pretty similar between the two plots, despite the fact that they used separate samples of data.\n",
        "\n",
        "Comparing the residual plots also yields a similar perspective:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0bx2vRkGP4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmaWhyNhLtld",
        "colab_type": "text"
      },
      "source": [
        "It's easier to see the degree of similarity by placing them on the same plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmgYNS7KGj-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hue='sample' subsets the data by sample and creates a single plot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87RVf8bOHUMI",
        "colab_type": "text"
      },
      "source": [
        "Although the slopes of the lines are slightly different, the model predictions do not vary too wildly from each other, which is good indication of a model variance that is not too high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4x3Zwd6MoeC",
        "colab_type": "text"
      },
      "source": [
        "So, what was the point of this exercise? This was a visual demonstration of a the bias and variance of a linear model with one slope parameter. The standards described above represent and incredible simplistic model. The bias and variance of a model usually depends on the complexity of a model:\n",
        "\n",
        "- Bias decreases with complexity.\n",
        "- Variance increases with complexity.\n",
        "\n",
        "This relationships is a rule of thumb rather than a formalism. Sometimes you can have a complex model with extremely high bias and variance. \n",
        "\n",
        "In our case, the model has:\n",
        "\n",
        "  - High bias because it doesn't fit the data particularly well. \n",
        "  - Low variance because it doesn't change much depending upon which points happen to be in the sample.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AttLIOfANMau",
        "colab_type": "text"
      },
      "source": [
        "What would a low bias, high variance model look like? Let's consider a linear model with fifteen terms, with each term being the predictor raised to a higher power. This is known as polynomial regression, in particular an fifteenth order polynomial represented by:\n",
        "\n",
        "$$\n",
        "y^{pred} = \\sum_{j=0}^{15} \\beta_{j}x^{j}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VC-SbeKPehM",
        "colab_type": "text"
      },
      "source": [
        "This is still a linear model (w.r.t. $\\beta$) but with added complexities due to considering higher order transformations of the predictor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX7wpzC5PgZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# col='sample' subsets the data by sample and creates two separate plots\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkSHLm_cRQAd",
        "colab_type": "text"
      },
      "source": [
        "What can we observe from the plots above?\n",
        "\n",
        "- The model seems to be attuned to the general shape (or pattern of the data).\n",
        "- Remember, the errors of prediction have a systematic component to it, and in our case the irreducible errors due to the natural variance of the data seems to quite large.\n",
        "- Because it fits the general shape of the data better than the simple linear model, we can loosely say it has lower *bias*.\n",
        "- However, the shapes of the model trained on the different samples data are wildly different. This is a clear indication of a large model variance. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyE75Ix2RfxV",
        "colab_type": "text"
      },
      "source": [
        "#### The Tradeoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg-XG5RJUW7R",
        "colab_type": "text"
      },
      "source": [
        "In general if the model is too simplistic and has very few parameters, it will not be able to account for all the variances in the data. On the other hand, if our model has too large a number of parameters then, it might start to model the errors in the data itself - thus giving a high variance. However because of the larger number of parameters, we may find ourselves with a model that is more able to explain the relationships in the data better. \n",
        "\n",
        "The objective of machine learning is to find a model specification that is subject to the a good balance of bias and variance. Thus, we aim to try to find the model with a complexity within a goldilocks zone:\n",
        "\n",
        "<figure> \n",
        "<img src=\"https://miro.medium.com/max/1124/1*RQ6ICt_FBSx6mkAsGVwx8g.png\">\n",
        "<div style=\"text-align:center;\">\n",
        "<figcaption > Source: Seema Singh </figcaption> </div>\n",
        "</figure>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u9HzuCTeBRl",
        "colab_type": "text"
      },
      "source": [
        "### How Well Does the Model Fit the data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sibybRQq6ODB",
        "colab_type": "text"
      },
      "source": [
        "So far, we have talked about bias and variance being the measures of performance; however a correct identification of bias and variance requires complete knowledge of data and problem at hand. \n",
        "\n",
        "As we do not have this luxury, there needs to be a way to quantify the fit of the model to the data (to measure a proxy of bias), as well as the generalisation error (to measure a proxy of the model variance). We will look at methods of measuring model fit first.\n",
        "\n",
        "One common way to evaluate the overall fit of a linear model is using the R-squared value. R-squared is the proportion of variance of the data explained, meaning the proportion of variance in the observed data that is explained by the model, or the reduction in error over the null model. (The null model just predicts the mean of the observed response, and thus it has an intercept and no slope.)\n",
        "\n",
        "It is defined by the following formula:\n",
        "\n",
        "  $$\n",
        "  R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}\n",
        "  $$\n",
        "\n",
        "The higher the $R^2$ the more indicative that the model has a better fit on the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs1cNwXc7y4L",
        "colab_type": "text"
      },
      "source": [
        "Let's calculate the R-squared value for our simple linear model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoPPcb4e7kbl",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise:\n",
        "\n",
        "Write a function to calculate the $R^2$ value of your model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSWLsmFz7stU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def r_squared():\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr8QTrR2IKOk",
        "colab_type": "text"
      },
      "source": [
        "You can use the built in methods in `statsmodels` and `sklearn` to compute the $R^2$ score too:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ds6VriuG_-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### STATSMODELS ###\n",
        "\n",
        "# print the R-squared value for the model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXWgcWkOHCGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### SCIKIT-LEARN ###\n",
        "\n",
        "# print the R-squared value for the model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD6EDpQxHPLn",
        "colab_type": "text"
      },
      "source": [
        "Is that a \"good\" R-squared value? It's hard to say. The threshold for a good R-squared value depends widely on the domain. Rather, it's more useful as a tool for comparing different models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3EieLP1HdM4",
        "colab_type": "text"
      },
      "source": [
        "#### Increasing Model Fit (and Caveats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGjF0ywaHicN",
        "colab_type": "text"
      },
      "source": [
        "The $R^2$ statistic usually increases every time you add an **independent** variable to the model.\n",
        "\n",
        "A regression model that contains more independent variables than another model can look like it provides a better fit merely because it contains more variables.\n",
        "\n",
        "Let's see an example below, we will consider the full linear model as compared to a linear with a single predictor variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxru5_3rI1bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### STATSMODELS ###\n",
        "\n",
        "# Fit the model\n",
        "\n",
        "\n",
        "# Check the rsquared\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnblAAMyJT22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### SCIKIT-LEARN ###\n",
        "\n",
        "# create X and y\n",
        "\n",
        "\n",
        "# instantiate and fit\n",
        "\n",
        "#Check the r-squared score\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHusaTPOQ96I",
        "colab_type": "text"
      },
      "source": [
        "When a model contains an excessive number of independent variables and polynomial terms, it becomes overly customized to fit the peculiarities and random noise in your sample rather than reflecting the entire population. Thus, blindly chasing a high $R^2$ value will lead to a low bias but high variance model. This means that the model might be tend be overfit to the training data and have a decreased capability for precise predictions in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkIu-GP1RpqB",
        "colab_type": "text"
      },
      "source": [
        "There is an alternative to R-squared called [adjusted R-squared](https://www.statisticshowto.com/adjusted-r2/) which penalizes model complexity (to control for overfitting), but it generally under-penalizes complexity - as such we will not go into the specifics here. We will explore a more robust method to diagnose overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00Xh3yO1R81e",
        "colab_type": "text"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPYI5RFvSovp",
        "colab_type": "text"
      },
      "source": [
        "So far, we have implemented Linear Regression on the following:\n",
        "\n",
        "1. Using a single predictor.\n",
        "2. Using all available predictors.\n",
        "\n",
        "Which led to varying results. \n",
        "\n",
        "There are many more statistics (which are out of scope of this tutorial) that have been used to evaluate a goodness of fit on the data but bear in mind that the statistics are point estimates that do not give a complete picture of the whether the model is a good fit.\n",
        "\n",
        "Many such statistics can be easily obtained from the Statsmodels model summary output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opMa2jJKTAJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### STATSMODELS ###\n",
        "\n",
        "# print a summary of the fitted model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewm4VnXNTDSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summary for the model with all predictors\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpt5qsikTGBm",
        "colab_type": "text"
      },
      "source": [
        "It can be surmised that the full Linear Regression model does better in terms of fitting the data but is this truly the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W2ovMOqU1aA",
        "colab_type": "text"
      },
      "source": [
        "## Generalisation and Feature Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLpkRUV3U25n",
        "colab_type": "text"
      },
      "source": [
        "We seek a model with a good feature set capable learning from the data and making the best predictions. In other words, the feature set should describe the data well, without attuning itself to any irrelevant noise. The question then becomes, how do I decide which features to include in a linear model to make it predict well?\n",
        "\n",
        "First up, how do we tell if it is able to predict well?\n",
        "\n",
        "So far, we have only seen and evaluated the models on the *observed* data. But the true indication of the model's performance only comes in evaluating on data that is unseen.\n",
        "\n",
        "In other words, does it generalise well? Or did the model simply memorise the data points it has seen?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NmMxYqCW_6m",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1Yp5lJSj2R",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "How do we then evaluate the model on unseen data? An obvious method would be to go and collect new data points. But if that is not possible, a trick we can use it to split the data into two. \n",
        "\n",
        "By splitting the data into what we call a training set and a testing set, we can train the model on the former and evaluate it on the latter. The testing or test set is a proxy to any future data that we want to make predictions on.\n",
        "\n",
        "This is paradigm provides more reliable estimate of out-of-sample error, and thus are better for choosing which of your models will best generalize to out-of-sample data - which in turns measures the variance of the model.\n",
        "\n",
        "More importantly, the idea of data splitting can be applied to any model to measure generalisation errors. Many of the statistics described above (in the model summary) apply only to linear models with certain assumptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXwtIUugX6i4",
        "colab_type": "text"
      },
      "source": [
        "#### Model Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjZOJfE0Y5cU",
        "colab_type": "text"
      },
      "source": [
        "Recall that we had come up with the following evaluation metrics when discussing the objective function:\n",
        "\n",
        "- **Mean Squared Error** (MSE) is the mean of the squared errors:\n",
        "\n",
        "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
        "\n",
        "- **Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
        "\n",
        "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtmxctYoshAl",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52RC5b_xsjfM",
        "colab_type": "text"
      },
      "source": [
        "Create a function that would be able to calculate:\n",
        "1. The MSE\n",
        "2. The rMSE\n",
        "\n",
        "Think about the parameters and output of the function, are they scalars, vectors, etc?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xxV6xIJsopz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse():\n",
        "  pass\n",
        "\n",
        "def rmse():\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfl96_UzZDe8",
        "colab_type": "text"
      },
      "source": [
        "There are utilities in `sklearn` to help us calculate these scoring mechanisms, instead of us having to manually do so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6npjzlhZs1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# define true and predicted response values\n",
        "\n",
        "# calculate MSE, RMSE\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC1TSgz1xAnx",
        "colab_type": "text"
      },
      "source": [
        "Checking the MSE and rMSE for the predictions give:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHJ7MQQQxFFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1QEBNPQbHjN",
        "colab_type": "text"
      },
      "source": [
        "Recall that the MSE \"punishes\" deviations from the true values. In practice, the rMSE is a more popular metric than MSE because rMSE is interpretable in scale of the actual target values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxrmfNulZ0Oq",
        "colab_type": "text"
      },
      "source": [
        "#### Train-Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbQM22zEaK_S",
        "colab_type": "text"
      },
      "source": [
        "Let's use train/test split with RMSE to see whether certain predictors should be kept in the model. The `sklearn` utilities allow us to easily split the data by random sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-CFX03peVd5",
        "colab_type": "text"
      },
      "source": [
        "*Without Splitting*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHXrwvRHcQLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# excluding newspaper\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PJqX-E1b-RW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# including newspaper\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BNPtKcPfIt5",
        "colab_type": "text"
      },
      "source": [
        "You can see that adding the extra term causes the rMSE to decrease (which is desirable) but only by a miniscule amount. Does this necessarily mean it is a good predictor?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWsH-P9CgXzy",
        "colab_type": "text"
      },
      "source": [
        "*With Splitting*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk1CIwwpguke",
        "colab_type": "text"
      },
      "source": [
        "The `train_test_split` function is a data transformer that splits the data randomly and returns a tuple of four values:\n",
        "\n",
        "- The training set of predictor variables : `X_train`\n",
        "- The training set of target variables : `y_train`\n",
        "- The test set of predictor variables : `X_test`\n",
        "- The test set of target variables : `y_test`\n",
        "\n",
        "You can specify the proportions of the splits the function parameters described in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JorhD9xSb_3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# excluding newspaper\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTC6tZX0cz-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# including newspaper\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "widaeo09dMBh",
        "colab_type": "text"
      },
      "source": [
        "In this case, the rMSE actually increased from adding the newspaper predictor.\n",
        "\n",
        "The above analysis makes the case that maybe the newspaper predictor has no bearing on the sales of the widgets.\n",
        "\n",
        "Remember, always test your model on data that have not been used for training! This is known as the *out-of-sample* data. By testing your model on *out-of-sample* data, you are evaluating the generalisation ability of the model.\n",
        "\n",
        "For the model to perform well, it needs to be tested on unseen data. This is known as the *generalisation* ability of the model. \n",
        "\n",
        "**To learn, is to generalise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaWBA5ORinnS",
        "colab_type": "text"
      },
      "source": [
        "#### Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3myuq2xjlrq",
        "colab_type": "text"
      },
      "source": [
        "Using the available data to perform machine learning is the preliminary step. However, there are many ways of improving the performance through clever manipulation of the data. This is known as feature engineering.\n",
        "\n",
        "More formally, feature engineering is defined as the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering is constantly used in practice and is a large domain in the field of applied machine learning.\n",
        "\n",
        "Ultimately, it goes without saying that the model's features influence the performance more than any other factor. In practice, the choice of models, along with the specification of the algorithm plays second fiddle to the information gain given by correct feature engineering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVVFd8e9lxw5",
        "colab_type": "text"
      },
      "source": [
        "Let's now attempt to create some features from the raw data. So far, all of our features have been numeric, let's also consider how we might handle categorical data in linear regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbZasFu5gFP9",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise: \n",
        "\n",
        "Create a new feature called budget, depending on how much money was spent in total on advertising. The budget should be classed as follows: \n",
        "\n",
        "- low for budget values below 33% of the observed values\n",
        "- medium for budget values between 33 - 67% of the observations\n",
        "- high for budget values for values at 67% and above of the observations\n",
        "\n",
        "This is called a discretization transformation. Explore either the pandas or sklearn documentation for help with this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "029fL2PBzTk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the histogram as a starting point\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UchRD6W_0Y5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Manually perform discretization using pd.qcut\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Assign a new column called budget to the predictors dataframe\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  print(predictors.loc[:,\"budget\"].unique()) #this should return an array containing ['high' 'low' 'medium']\n",
        "except NameError:\n",
        "  pass\n",
        "except KeyError:\n",
        "  print(\"Budget Column Not Found!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNrLxQlUkqVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Alternative method using the sklearn build in discretizer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myPp95WfmvLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check if both answers are equivalent\n",
        "\n",
        "\n",
        "# There is a discrepancy at one point\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xdh3CWD43_W",
        "colab_type": "text"
      },
      "source": [
        "The point of discrepancy is at a boundary. Since there is only one point, we can arbitrarily assign it to either class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrhQ_7r-DLOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-303BbJQBz17",
        "colab_type": "text"
      },
      "source": [
        "### Encoding categorical variables for modelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C44QH1adC_eB",
        "colab_type": "text"
      },
      "source": [
        "When building a linear model, we have to represent the categorical variables in a numerical format. This is because by fitting the data to the model we attempt to perform gradient descent to minimise the SSE, and by doing so we require that all the predictor values to be numeric. \n",
        "\n",
        "There are many ways to perform this encoding, one obvious way is to represent each categorical value as a number.\n",
        "\n",
        "However, we can't simply code it as 0=low, 1=medium, 2=high because that would imply a scale in the relationship between the categories, and that a 'high' value is somehow \"twice\" the 'low' value. While this **may** be true, by imposing this scaling we may be introducing extra assumptions into the model which might constrain it's ability to model the data well.\n",
        "\n",
        "For us to encode these variables into numerical values, we can use *one-hot-encoding*. In this procedure, we create additional dummy variables to represent the presence of each category. Let's explore how to do this using pandas:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwm820AfDRlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create three dummy variables using get_dummies\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZjjzBLLJvnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create three dummy variables using get_dummies, then exclude the first dummy column\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2kKogBEKH9A",
        "colab_type": "text"
      },
      "source": [
        "Here is how we interpret the coding:\n",
        "- **high** is coded as budget_high=1 and the rest of the budget columns are 0\n",
        "- **low** is coded as budget_low=1 and the rest of the budget columns are 0\n",
        "- **medium** is coded budget_medium=1 and the rest of the budget columns are 0\n",
        "\n",
        "\n",
        "Let's now add these dummy variables onto the original DataFrame, and then include them in the linear regression model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zWkXWgVKbu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concatenate the dummy variable columns onto the DataFrame (axis=0 means rows, axis=1 means columns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuxt3c_nLwGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data set into train and testing sets\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZGhveP1bL5i",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUrE7MLNM4oU",
        "colab_type": "text"
      },
      "source": [
        "The model yields a rMSE lower than that of the basic linear model with the raw predictors (~1.404), but more than that of the linear model without factoring in the *newspaper* predictor. An exercise would be to compare the out-of-sample rMSE of the models with different permutations of the predictors to see which set of predictors yields the linear model with the best performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMHbu1SeLKZJ",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNZeBUCpLMaY",
        "colab_type": "text"
      },
      "source": [
        "Try different model permutations and compute the rMSE for each model, which models do you think perform the best?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC0mOqGLpwI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvLFzEwbl6nI",
        "colab_type": "text"
      },
      "source": [
        "You can do it manually, but there exists a function in sklearn called an RFE which recursively eliminates features from a model based on the the model's scoring function.  This technique begins by building a model on the entire set of predictors parameters and computing an importance score for each predictor.\n",
        "\n",
        "The least important predictor(s) are then removed, the model is re-built, and importance scores are computed again. \n",
        "\n",
        "The process stops when the desired number of predictors are selected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lduWj9cDLa5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RFE Method\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZXlzaefcHzP",
        "colab_type": "text"
      },
      "source": [
        "So the model with the best out of sample predictions would be one determined by the following set of predictors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYBlCwgD_Zpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADjCheJtXBos",
        "colab_type": "text"
      },
      "source": [
        "### What Didn't We Cover?\n",
        "\n",
        "- Detecting collinearity, i.e whether predictors rely on each other\n",
        "- Diagnosing model fit using statistical theory\n",
        "- Transforming features to fit non-linear relationships\n",
        "- Interaction terms\n",
        "- More statistical assumptions of linear regression\n",
        "\n",
        "You could certainly go very deep into linear regression, and learn how to apply it really well. It is an excellent way to **start your modeling process** when working a regression problem. Again, in the words of Yasser Abu Mostafa, a linear model will take you far.\n",
        "\n",
        "However, it is limited by the fact that it can only make good predictions if there is a **linear relationship** between the features and the response, which is why more complex methods (but often with higher variance and lower bias) will often outperform linear regression.\n",
        "\n",
        "Therefore, by understanding linear regression conceptually, as well as its strengths and weaknesses, you will have gained the understanding of creating a machine learning model to solve a prediction problem - which can be extended to other models in practice."
      ]
    }
  ]
}